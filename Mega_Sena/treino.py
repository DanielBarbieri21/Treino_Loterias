import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.multioutput import MultiOutputClassifier
from sklearn.dummy import DummyClassifier
from xgboost import XGBClassifier
import joblib

# ================================
# üìÇ Carregar dados
# ================================
ARQUIVO_TREINO = "mega_sena.xlsx"  # seu arquivo de Mega-Sena
df = pd.read_excel(ARQUIVO_TREINO)

# Filtrar apenas n√∫meros v√°lidos (1 a 60)
df_numeros = df.applymap(
    lambda x: int(x) if str(x).isdigit() and 1 <= int(x) <= 60 else np.nan
).dropna(how="all")

# Converter y em bin√°rio (one-hot encoding 1-60)
y_binario = pd.DataFrame(0, index=np.arange(len(df_numeros)), columns=range(1, 61))
for i, linha in enumerate(df_numeros.values):
    for n in linha:
        if not pd.isna(n):
            y_binario.at[i, int(n)] = 1

# X simples: √≠ndice dos sorteios
X = np.arange(len(df_numeros)).reshape(-1, 1)

# ================================
# üîÄ Divis√£o treino/teste
# ================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y_binario, test_size=0.2, random_state=42
)

# ================================
# ü§ñ Modelos
# ================================
dummy = MultiOutputClassifier(DummyClassifier(strategy="most_frequent"))
dummy.fit(X_train, y_train)
y_pred_dummy = dummy.predict(X_test)

rf = MultiOutputClassifier(RandomForestClassifier(n_estimators=200, random_state=42))
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

xgb = MultiOutputClassifier(XGBClassifier(
    eval_metric="logloss", use_label_encoder=False, random_state=42
))
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

# ================================
# üìä Fun√ß√£o de relat√≥rio
# ================================
def relatorio_metricas(y_true, y_pred, nome):
    report = classification_report(
        y_true, y_pred, target_names=[str(i) for i in range(1, 61)], output_dict=True
    )
    df_report = pd.DataFrame(report).transpose()
    df_report.index.name = f"M√©tricas - {nome}"
    return df_report

df_dummy = relatorio_metricas(y_test, y_pred_dummy, "Dummy")
df_rf = relatorio_metricas(y_test, y_pred_rf, "RandomForest")
df_xgb = relatorio_metricas(y_test, y_pred_xgb, "XGBoost")

# ================================
# üé≤ Fun√ß√£o para gerar previs√µes probabil√≠sticas
# ================================
def gerar_previsoes_probabilisticas(modelo, n_previsoes=10, n_numeros=6, n_total=60):
    previsoes = []

    # Probabilidades m√©dias de cada n√∫mero
    probs = np.zeros(n_total)
    for i, clf in enumerate(modelo.estimators_):
        proba = clf.predict_proba([[len(df_numeros)]])[0]
        if len(proba) > 1:
            probs[i] = proba[1]

    probs = probs / probs.sum()  # normalizar soma=1

    for _ in range(n_previsoes):
        numeros = np.random.choice(np.arange(1, n_total+1), size=n_numeros, replace=False, p=probs)
        numeros = [int(n) for n in numeros]
        previsoes.append(sorted(numeros))

    return previsoes

# Modelo final
modelo_final = xgb
joblib.dump(modelo_final, "modelo_mega_xgb.pkl")

previsoes = gerar_previsoes_probabilisticas(modelo_final, n_previsoes=10, n_numeros=6, n_total=60)

# ================================
# üåê Interface Streamlit
# ================================
st.title("üé≤ Previs√£o Mega-Sena com Machine Learning")

# Frequ√™ncia dos n√∫meros
st.subheader("üìä Frequ√™ncia dos n√∫meros no hist√≥rico")
fig, ax = plt.subplots(figsize=(12, 4))
df_numeros.stack().value_counts().sort_index().plot(kind="bar", ax=ax)
ax.set_xlabel("N√∫mero")
ax.set_ylabel("Frequ√™ncia")
st.pyplot(fig)

# Relat√≥rios
st.subheader("üìë Relat√≥rios de Classifica√ß√£o")
st.write("**Dummy Classifier (baseline):**")
st.dataframe(df_dummy.style.format("{:.2f}"))
st.write("**RandomForest:**")
st.dataframe(df_rf.style.format("{:.2f}"))
st.write("**XGBoost:**")
st.dataframe(df_xgb.style.format("{:.2f}"))

# Matriz de confus√£o (classe 1 exemplo)
st.subheader("üîç Matriz de Confus√£o - RandomForest (classe 1)")
cm = confusion_matrix(y_test.iloc[:, 0], y_pred_rf[:, 0])
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
ax.set_title("Matriz de Confus√£o - Classe 1")
st.pyplot(fig)

# Curva ROC (classe 1)
st.subheader("üìà Curva ROC - XGBoost (classe 1)")
y_score = modelo_final.estimators_[0].predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test.iloc[:, 0], y_score)
roc_auc = roc_auc_score(y_test.iloc[:, 0], y_score)

fig, ax = plt.subplots()
ax.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
ax.plot([0, 1], [0, 1], linestyle="--")
ax.set_xlabel("False Positive Rate")
ax.set_ylabel("True Positive Rate")
ax.set_title("Curva ROC (classe 1)")
ax.legend()
st.pyplot(fig)

# Previs√µes probabil√≠sticas
st.subheader("üéØ 10 Previs√µes Probabil√≠sticas (6 n√∫meros cada)")
for i, prev in enumerate(previsoes, start=1):
    st.write(f"**Previs√£o {i}:** {prev}")
